{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ea6211",
   "metadata": {},
   "source": [
    "# Ling 380 - Week 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec2791",
   "metadata": {},
   "source": [
    "# Processing survey data\n",
    "\n",
    "Survey answers are typically of the style below, with a very specific question and a [Likert scale](https://en.wikipedia.org/wiki/Likert_scale) that forces the answers into clearly defined categories.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./img/Likert.png\" style=\"width:450px;\"\n",
    "         alt=\"supervised classification\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "This is easy to process, as the answers can be tabulated. Some surveys, however, allow free-text answers. This is the case in the type of question you get at the end of a survey, something like \"Anything else you'd like to tell us?\". It's also common in focus groups and submissions in public consultation processes. These can go from one-word answers and short sentences to long paragraphs. That's the type of unstructured data that you need text analysis for!\n",
    "\n",
    "There are many ways we can analyze and summarize the information. we'll focus on...\n",
    "\n",
    "* visualization with word clouds (lemma, stopwords, etc.)\n",
    "* ner recognition\n",
    "* redaction?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6ba96",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "There are many public surveys and sources of data (see at the end for more links). Here, we will work with the [Democracy Checkup](https://odesi.ca/en/details?id=/odesi/doi__10-5683_SP3_TEKM3T.xml) distributed by [Odesi](https://odesi.ca/en), a Canadian consortium that holds social science data. This is a survey of Canadian attitudes about democratic values, public policies, and current issues: \n",
    "\n",
    "* Harell, Allison; Stephenson, B. Laura; Rubenson, Daniel; Loewen, Peter John, 2023, \"Democracy Checkup, 2022. Canada\", https://doi.org/10.5683/SP3/TEKM3T, Borealis, V1, UNF:6:ufqbMikbXcaHqVhbaEXR3w== (fileUNF)\n",
    "\n",
    "The data contains many different variables, most of them numeric or on a scale with fixed values to choose from. But some of the values are free-form text answers; we'll study those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf7dfb",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87f021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\dueck\\anaconda3\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\dueck\\anaconda3\\lib\\site-packages (from wordcloud) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dueck\\anaconda3\\lib\\site-packages (from wordcloud) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dueck\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dueck\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dueck\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dueck\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dueck\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dueck\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dueck\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dueck\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl (301 kB)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n"
     ]
    }
   ],
   "source": [
    "# run this only once, to install wordcloud\n",
    "\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7256ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe696a",
   "metadata": {},
   "source": [
    "## Import and examine the data\n",
    "\n",
    "The data is huge: 9,829 rows (answers) and 501 questions (columns). As you can see from the output of `df.head()`, most of the data is numerical, so we'll work only with text columns. Luckily, those columns always end with the string \"TEXT\", so we can use pandas to extract them. For instance: \n",
    "\n",
    "* dc22_vote_choice_6_TEXT -- what party the person intends to vote for (in Dec 2022)\n",
    "* dc22_soc_media_use_9_TEXT -- what social media the person uses\n",
    "* dc22_language_3_TEXT -- what language the person speaks\n",
    "* etc.\n",
    "\n",
    "You can use the [Data explorer](https://borealisdata.ca/data-explorer/?siteUrl=https:%2F%2Fborealisdata.ca&dvLocale=en&fileId=659110) for this dataset to check the actual questions for each of these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2cbe480",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/cora-cdem-2022_F1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/cora-cdem-2022_F1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/cora-cdem-2022_F1.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/cora-cdem-2022_F1.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f7ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ea124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which columns contain the word \"TEXT\" using a regular expression\n",
    "\n",
    "print(df.filter(regex='TEXT').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe, 'df_text' that contains only those columns\n",
    "\n",
    "df_text = df.filter(regex='TEXT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ac9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542da70c",
   "metadata": {},
   "source": [
    "## Working with one column\n",
    "\n",
    "We will work with one of the columns, 'dc22_vote_choice_6_TEXT', where the question was: \"If a federal election were held today, which party would you be most likely to vote for?\" \n",
    "\n",
    "We will first extract data from that column to a pandas series. When extracting, we will drop any empty values (NaN) with `dropna()` and the columns that contain '-99', which means the person did not answer this question.\n",
    "\n",
    "Then, we put that into a string variable and lowercase it (so that \"People's Party\" and \"People's party\" are considered the same). But then we realize actually that the word \"party\" occurs here a lot, so we'll simply remove it with the stopwords. \n",
    "\n",
    "Finally, we have a somewhat clean string of words that we can send to WordCloud to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the values are '-99' (unanswered)\n",
    "# but you can sort other values to find the most frequent in a column\n",
    "\n",
    "df_text['dc22_vote_choice_6_TEXT'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c16b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice = df_text['dc22_vote_choice_6_TEXT'].dropna().loc[df_text['dc22_vote_choice_6_TEXT'].str.contains('99') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a pandas series, so we'll convert it to a string\n",
    "type(vote_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_str = ', '.join(vote_choice.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea76f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the contents of stop_words\n",
    "\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d55bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the word \"party\"\n",
    "\n",
    "stop_words.append('party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d481ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that: 1. tokenizes, 2. lowercases and\n",
    "# 3. removes stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    cleaned_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_clean = clean_text(vote_choice_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10e60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9f359",
   "metadata": {},
   "source": [
    "## Get frequencies\n",
    "\n",
    "You'll see below that WordCloud randomizes the font size of the output. But sometimes we want that to be meaningful, reflecting how frequent the word is. To get that information, we will use NLTK's FreqDist and generate a dictionary when we clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30608cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that: 1. tokenizes, 2. lowercases,\n",
    "# 3. removes stopwords and 4. counts the words\n",
    "\n",
    "def clean_text_freq(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    cleaned_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    freq_dist = FreqDist(cleaned_words)\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9427153",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_dict = clean_text_freq(vote_choice_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8518eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e4896",
   "metadata": {},
   "source": [
    "## Word clouds\n",
    "Word clouds are popular and can look very cool in a report. They can also be misleading if the size of the font does not correspond to the frequency of the word in the data, so you should use them with caution.  \n",
    "\n",
    "We'll use the [Wordcloud](https://github.com/amueller/word_cloud/) library, which we imported above. There are many options for how to do this. Here are two possibilities, changing the shape and the background colour. Check out the [example gallery](https://amueller.github.io/word_cloud/auto_examples/index.html#example-gallery) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ff54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "x, y = np.ogrid[:300, :300]\n",
    "\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "# this actually generates the word cloud\n",
    "wc = WordCloud(background_color=\"white\", repeat=True, mask=mask)\n",
    "wc.generate(vote_choice_clean)\n",
    "\n",
    "# and this displays it\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c26f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(vote_choice_clean)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigger\n",
    "wordcloud = WordCloud().generate(vote_choice_clean)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d3b2f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## With the frequency dictionary\n",
    "Recall that above we created a dictionary of the frequency of each word. We can use it to display relative to frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05caf6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate_from_frequencies(vote_choice_dict)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041832e3",
   "metadata": {},
   "source": [
    "# Named entities\n",
    "\n",
    "Another thing you may want to do with survey results is extract the named entities mentioned in the text. Remember that we can do this with spaCy (see Week6). \n",
    "\n",
    "We will use the variable `vote_choice_str` from earlier, which is simply the running text of the column about vote choice. We process it with spaCy (which was imported at the top) and we can print the entities that are of type 'ORG', which should correspond to a political party. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb99560",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_doc = nlp(vote_choice_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all entities and their label\n",
    "# note that there are errors here\n",
    "\n",
    "for ent in vote_choice_doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264aec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the ORG entities\n",
    "\n",
    "for ent in vote_choice_doc.ents:\n",
    "    if ent.label_ == \"ORG\":\n",
    "        print(ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ecf23a",
   "metadata": {},
   "source": [
    "# Redacting documents\n",
    "\n",
    "This idea comes from an [NLP notebook on redacting names](https://github.com/Jcharis/Natural-Language-Processing-Tutorials/blob/master/NLP_with_SpaCy/Automatic%20Redaction%20%20%26%20Sanitization%20of%20Document%20Using%20Spacy%20NER.ipynb). Once you have named entities identified (hopefully accurately), you can also use the NER output to redact any personal information. For instance, you can identify all the person's names and remove them or replace them with something like 'REDACTED'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65bfbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_names(text):\n",
    "    doc = nlp(text)\n",
    "    redacted_sentences = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.ent_type_ == 'PERSON':\n",
    "            redacted_sentences.append(\"[REDACTED]\")\n",
    "        else:\n",
    "            redacted_sentences.append(token.text)\n",
    "    \n",
    "    return ' '.join(redacted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_redacted = sanitize_names(vote_choice_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622391e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_redacted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32edbae",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have learned about processing and aggregating survey data. This notebook has used some concepts we have learned previously:\n",
    "\n",
    "* Tokenization\n",
    "* Lowercasing\n",
    "* Removing stopwords\n",
    "* Creating a function to clean text\n",
    "* Reading in and manipulating data in pandas\n",
    "\n",
    "New information:\n",
    "\n",
    "* Creating word clouds\n",
    "* Using NER (named entity recognition) to redact documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
